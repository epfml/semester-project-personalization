\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amssymb,amsmath,amsthm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algpseudocode}  % http://mirror.kumi.systems/ctan/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{graphicx}

\usepackage{lipsum}         % for dummy text
\usepackage{xspace}         % for at the end of macros
\usepackage{xargs}          % defines \newcommandx
\usepackage{mathtools}
\usepackage{bm}
\usepackage[dvipsnames]{xcolor} % defines \textcolor
\usepackage{tabularx}       % like this for tables
\usepackage{wrapfig}        % left- and right-floating figures+tables

\usepackage[export]{adjustbox}


\usepackage{tikz}           % for drawing
\usepackage{ifthen}
\usepackage{enumitem}
\usepackage{cleveref}
\usetikzlibrary{patterns}
\usetikzlibrary{positioning}

\newcommand\includegraphicscrop[1]{%
\immediate\write18{pdfcrop -hires #1.pdf #1-crop.pdf}%
\includegraphics{#1-crop.pdf}%
}
     
\input{macros}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
% \newcommand{\gset}{\ensuremath{\cV_{\mathpzc{R}}}}
% \newcommand{\bset}{\ensuremath{\cV_{\mathpzc{B}}}}
\newcommand{\gset}{\ensuremath{{\mathpzc{G}}}}
\newcommand{\bset}{\ensuremath{{\mathpzc{B}}}}
\newcommand{\gcset}{\ensuremath{{\mathpzc{G_c}}}}
\newcommand{\bcset}{\ensuremath{{\mathpzc{B_c}}}}
\newcommand{\gfset}{\ensuremath{{\mathpzc{G_f}}}}
\newcommand{\bfset}{\ensuremath{{\mathpzc{B_f}}}}
\newcommand{\cpG}{{{\mathsf{G}}}}
\newcommand{\cpH}{{{\mathsf{H}}}}
\newcommand{\cpC}{{{\mathsf{C}}}}


\title{}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

%\begin{abstract}
%    
%\end{abstract}
\section{Introduction}

In the federated learning and decentralized learning, $n$ participants collaborate to train a global model $\xx$ over their joint objectives $\min_{\xx} \frac{1}{n}\sum_{i=1}^n f_i(\xx)$.
Compared to models trained on individual data silos, this model achieves overall better performance on dataset. 
There is no guarantee that it performs better than standalone training on some workers. 
Personalized federated learning is one way to address this problem. 
% 

\section{Shared setting}
\begin{assumption}[$L$-smoothness]\label{a:smooth}
  For $i\in[n]$, $f_i$ is $L$-smooth.
\end{assumption}
\begin{assumption}[Lower bound]\label{a:lower-bound}
  For $i\in[n]$, $f_i$ is lower bounded by $f_i^\star$.
\end{assumption}

\section{Fully connected graph}
\subsection{Problem formulation}
In this section, we assume all workers share same stationary points or minimizers. That is, the communication graph is static and fully connected, i.e.
\begin{assumption}[Strong growth condition]\label{a:strong-growth}
  For $i\in[n]$ and $\xx\in\R^d$
  \begin{align*}
    \norm*{\nabla f_i(\xx) - \nabla \bar{f}(\xx)}_2 \le M \norm*{\nabla \bar{f}(\xx)}_2.
  \end{align*}
\end{assumption}
We formulate the following optimization objective
\begin{align*}
  \min_{X\in\mathbb{R}^{d\times n}}&  \frac{1}{n} \sum_{i=1}^n f_i(x_i) + \frac{\rho}{2} \sum_{i<j} \norm*{x_{i} - x_{j}}_2^2.
\end{align*}
We optimize the objective with gradient descent with initialization $\xx_i^0=\bar{\xx}^0$
\begin{equation}\label{eq:x:gd:1}
    \xx_i^{t+1}=\xx_i^t - \eta \left(
      \nabla f_i(\xx^t_i) + \rho \sum_{k=1}^n (\xx^t_i - \xx^t_k)
    \right).
\end{equation}

\subsection{Proof Sketch}
Let us define the averaged iterate $\bar{\xx}=\sum_{k=1}^n \xx_k$ and averaged objective $\bar{f}(\cdot)=\frac{1}{n}\sum_{i=1}^n f_i(\cdot)$. Then $\bar{\xx}$ has the following recursive relation
\begin{equation}\label{eq:barx:gd:1}
  \bar{\xx}^{t+1} = \bar{\xx}^t - \frac{\eta}{n} \sum_{i=1}^n \nabla f_i(\xx^t_i)
\end{equation}

\begin{lemma}[Sufficient Decrease]\label{lemma:sd:1}
  Suppose \Cref{a:smooth,a:lower-bound} holds. Then the recursion~\eqref{eq:x:gd:1} satisfies
  \begin{align*}
    \frac{1}{T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2
    \le \frac{2L(\bar{f}(\xx^{0}) - \bar{f}^\star)}{T}
    + \frac{1}{T} \sum_{t=0}^{T-1} \norm*{L(\bar{\xx}^{t}-\bar{\xx}^{t+1}) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2.
  \end{align*}
\end{lemma}
\begin{proof}
  By the $L$-smoothness \Cref{a:smooth} of $\bar{f}$, we have
  \begin{align*}
    \bar{f}(\bar{\xx}^{t+1}) \le& \bar{f}(\bar{\xx}^{t}) - \left\langle\nabla \bar{f}(\bar{\xx}^{t}), \bar{\xx}^{t} - \bar{\xx}^{t+1} \right\rangle + \frac{L}{2} \norm{\bar{\xx}^{t}-\bar{\xx}^{t+1}}_2^2.
  \end{align*}
  The linear term can be expanded as follows
  \begin{align*}
    - \left\langle\nabla \bar{f}(\bar{\xx}^{t}), \bar{\xx}^{t} - \bar{\xx}^{t+1} \right\rangle 
    =- \frac{1}{2L} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2 - \frac{L}{2} \norm{\bar{\xx}^{t}-\bar{\xx}^{t+1}}_2^2
    + \frac{L}{2} \norm*{\bar{\xx}^{t}-\bar{\xx}^{t+1} - \frac{1}{L}\nabla \bar{f}(\bar{\xx}^{t})}_2^2.
  \end{align*}
  Incorporating this into the above inequality, we have 
  \begin{align*}
    \bar{f}(\bar{\xx}^{t+1}) \le& \bar{f}(\bar{\xx}^{t}) - \frac{1}{2L} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2
    + \frac{L}{2} \norm*{\bar{\xx}^{t}-\bar{\xx}^{t+1} - \frac{1}{L}\nabla \bar{f}(\bar{\xx}^{t})}_2^2.
  \end{align*}
  By averaging the above inequality over $t=0$ to $T-1$, we derive
  \begin{align*}
    \frac{1}{T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2
    \le \frac{2L(\bar{f}(\xx^{0}) - \bar{f}^\star)}{T}
    + \frac{1}{T} \sum_{t=0}^{T-1} \norm*{L(\bar{\xx}^{t}-\bar{\xx}^{t+1}) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2.
  \end{align*}
\end{proof}
Before analyzing $\frac{1}{T} \sum_{t=0}^{T-1} \norm*{L(\bar{\xx}^{t}-\bar{\xx}^{t+1}) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2$, we first look into the consensus distance $\norm{\xx_i - \bar{\xx} }_2$ over time.
\begin{lemma}[Consensus distance]\label{lemma:consensus-distance:1}
  Suppose that \Cref{a:smooth} holds.
  If we take $$n\rho\eta\le 1 \text{ and } n\rho\eta\ge\max\left\{\frac{1}{2}, 12(1+M^2)L^2\eta^2\right\},$$
  then the consensus distance satisfies
  \begin{align*}
    \frac{1}{nT}\sum_{i=1}^n\sum_{t=0}^{T-1} \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    \le& \frac{1}{2L^2T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^t) }_2^2.
  \end{align*}
\end{lemma}
\begin{proof}
  The consensus distance is the distance between ~\eqref{eq:x:gd:1} and~\eqref{eq:barx:gd:1}
  \begin{align*}
    \norm*{\xx_i^{t+1} - \bar{\xx}^{t+1}}_2^2
    =& \norm*{(1-n\rho\eta)(\xx_i^{t} - \bar{\xx}^{t})-\eta\left(\nabla f_i(\xx_i^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)\right) }_2^2 \\
    \le&2(1-n\rho\eta)^2 \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    + 2\eta^2\norm*{\nabla f_i(\xx_i^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)}_2^2 \\
    \le&(1-n\rho\eta) \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    + 2\eta^2\norm*{\nabla f_i(\xx_i^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)}_2^2
  \end{align*}
  where we pick $\rho,\eta$ such that $n\rho\eta \ge \frac{1}{2}$ and $n\rho\eta\le 1$. The second term can be bounded as follows 
  \begin{align*}
    &\norm*{\nabla f_i(\xx_i^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)}_2^2\\
    =& \norm*{\nabla f_i(\xx_i^t) \pm \nabla f_i(\bar{\xx}^t) \pm \nabla \bar{f}(\bar{\xx}^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)}_2^2 \\
    \le& 3 \norm*{\nabla f_i(\xx_i^t) - \nabla f_i(\bar{\xx}^t)}_2^2 + 3 \norm*{\nabla f_i(\bar{\xx}^t) - \nabla \bar{f}(\bar{\xx}^t)}_2^2 + 3 \norm*{\nabla \bar{f}(\bar{\xx}^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)}_2^2 \\
    \le& 3 L^2 \norm*{\xx_i^t - \bar{\xx}^t}_2^2 + 3 M^2 \norm*{ \nabla \bar{f}(\bar{\xx}^t) }_2^2 + \frac{3}{n} \sum_{k=1}^n \norm*{\nabla f_k(\bar{\xx}^t) - \nabla f_k(\xx_k^t)}_2^2 \\
    \le& 3 L^2 \norm*{\xx_i^t - \bar{\xx}^t}_2^2 + 3 M^2 \norm*{ \nabla \bar{f}(\bar{\xx}^t) }_2^2 + \frac{3L^2}{n} \sum_{k=1}^n \norm*{\xx_k^t - \bar{\xx}^t}_2^2
  \end{align*}
  where we use \Cref{a:smooth,a:strong-growth} in the second inequality. Now we average over $i=1,\ldots,n$
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n\norm*{\nabla f_i(\xx_i^t) - \frac{1}{n}\sum_{k=1}^n\nabla f_k(\xx_k^t)}_2^2
    \le&  3 M^2 \norm*{ \nabla \bar{f}(\bar{\xx}^t) }_2^2 + \frac{6L^2}{n} \sum_{k=1}^n \norm*{\xx_k^t - \bar{\xx}^t}_2^2.
  \end{align*}
  Now the consensus distance at time $t+1$ can be bounded as follows
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \norm*{\xx_i^{t+1} - \bar{\xx}^{t+1}}_2^2
    \le& \frac{1-n\rho\eta}{n}\sum_{i=1}^n \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2 \\
    &+ 2\eta^2 \left(
      3M^2 \norm*{\nabla \bar{f}(\bar{\xx}^t) }_2^2 + \frac{6L^2}{n} \sum_{k=1}^n \norm*{\xx_k^t - \bar{\xx}^t}_2^2
    \right) \\
    =& \left(1-n\rho\eta+ 12L^2\eta^2 \right) \frac{1}{n}\sum_{i=1}^n \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    + 6M^2\eta^2 \norm*{\nabla \bar{f}(\bar{\xx}^t) }_2^2.
  \end{align*}
  By taking $n\rho\eta\ge 12 (1+M^2) L^2\eta^2$ we have hat
  \begin{align*}
    \frac{1}{n}\sum_{i=1}^n \norm*{\xx_i^{t+1} - \bar{\xx}^{t+1}}_2^2
    \le& \left(1-12M^2L^2\eta^2\right) \frac{1}{n}\sum_{i=1}^n \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    + 6M^2\eta^2 \norm*{\nabla \bar{f}(\bar{\xx}^t) }_2^2
  \end{align*}
  Sum the above inequality over $t=0$ to $T-2$ and add $x_i^0-\bar{\xx}^0=0$ to both sides and divide by $T$
  \begin{align*}
    \frac{1}{nT}\sum_{i=1}^n\sum_{t=0}^{T-1} \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    \le& \frac{1}{2L^2T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^t) }_2^2.
  \end{align*}
\end{proof}
Now we estimate $\norm*{L(\bar{\xx}^{t}-\bar{\xx}^{t+1}) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2$ in \Cref{lemma:sd:1}.
\begin{theorem}
  Suppose that \Cref{a:smooth,a:lower-bound,a:strong-growth} hold. If we set $\eta\le \frac{1}{\sqrt{24(1+M^2)}L}$ and $\rho=\frac{1}{n\eta}$, then the recursion~\eqref{eq:x:gd:1} ensures that
  \begin{align*}
    \frac{1}{T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2
    \le \frac{4(\bar{f}(\xx^{0}) - \bar{f}^\star)}{\eta T}.
  \end{align*}
  All iterates eventually reach consensus, i.e.
  \begin{align*}
    \frac{1}{nT}\sum_{i=1}^n\sum_{t=0}^{T-1} \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    \le&  \frac{2(\bar{f}(\xx^{0}) - \bar{f}^\star)}{L^2\eta T}.
  \end{align*}
\end{theorem}
\begin{proof}
  Now we use the iteration of~\eqref{eq:barx:gd:1} and the convexity of $\norm{\cdot}_2^2$ and smoothness \Cref{a:smooth} and $L\eta\le1$
  \begin{align*}
    \norm*{L(\bar{\xx}^{t}-\bar{\xx}^{t+1}) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2
    =& \norm*{\frac{L \eta}{n} \sum_{i=1}^n \nabla f_i(\xx_i^t) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2 \\
    \le& L\eta\norm*{\frac{1}{n}\sum_{i=1}^n \nabla f_i(\xx_i^t) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2 + (1-L\eta) \norm*{\nabla \bar{f} (\bar{\xx}^{t})}_2^2 \\
    \le& \frac{L^3\eta}{n}\sum_{i=1}^n\norm*{\xx_i^t - \bar{\xx}^{t}}_2^2 + (1-L\eta) \norm*{\nabla \bar{f} (\bar{\xx}^{t})}_2^2.
  \end{align*}
  By averaging the time $t=0$ to $T-1$ and using \Cref{lemma:consensus-distance:1}, we have
  \begin{align*}
    \frac{1}{T}\sum_{t=0}^{T-1}\norm*{L(\bar{\xx}^{t}-\bar{\xx}^{t+1}) - \nabla \bar{f}(\bar{\xx}^{t})}_2^2
    \le&
    \frac{L\eta}{2 T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^t) }_2^2 
    + \frac{1-L\eta}{T}\sum_{t=0}^{T-1} \norm*{\nabla \bar{f} (\bar{\xx}^{t})}_2^2 \\
    =&
    \frac{1-\frac{L\eta}{2}}{T}\sum_{t=0}^{T-1} \norm*{\nabla \bar{f} (\bar{\xx}^{t})}_2^2.
  \end{align*}
  Apply the above inequality into \Cref{lemma:sd:1}
  \begin{align*}
    \frac{1}{T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2
    \le \frac{2L(\bar{f}(\xx^{0}) - \bar{f}^\star)}{T}
    + \frac{1-\frac{L\eta}{2}}{T}\sum_{t=0}^{T-1} \norm*{\nabla \bar{f} (\bar{\xx}^{t})}_2^2.
  \end{align*}
  This gives the following bound on the convergence rate of gradient norm
  \begin{align*}
    \frac{1}{T} \sum_{t=0}^{T-1} \norm*{\nabla \bar{f}(\bar{\xx}^{t})}_2^2
    \le \frac{4(\bar{f}(\xx^{0}) - \bar{f}^\star)}{T\eta}.
  \end{align*}
  In order to minimize the right hand side, we maximize the stepsize $\eta$  that satisfy the following constraints
  \begin{align*}
    n\rho\eta\le 1 \text{ and } n\rho\eta\ge\max\left\{\frac{1}{2}, 12(1+M^2)L^2\eta^2\right\}, 
    \text{ and } \eta\le \frac{1}{L}.
  \end{align*}
  We simplify the problem by taking $\eta\le \frac{1}{\sqrt{24(1+M^2)}L}<\frac{1}{L}$ so that $n\rho\eta\in[\frac{1}{2}, 1]$. We can choose $\rho=\frac{1}{n\eta}$ to satisfy the constraint. By \Cref{lemma:consensus-distance:1}, we know that
  \begin{align*}
    \frac{1}{nT}\sum_{i=1}^n\sum_{t=0}^{T-1} \norm*{\xx_i^{t} - \bar{\xx}^{t}}_2^2
    \le& \frac{2(\bar{f}(\xx^{0}) - \bar{f}^\star)}{L^2\eta T}.
  \end{align*}
\end{proof}

\newpage
\appendix

\subsection{Algorithm}
We use ADMM type of optimization strategies.

\begin{enumerate}
  \item Update $X$ by gradient descent.
  \begin{align}
    \partial_{x_{i,s}} F =& \frac{1}{n}\partial_{x_{i,s}} f_i(x^t_{i,s},x^t_{i,p}) + \sum_{j=1}^n w^t_{ij} \text{sign}(j-i) \lambda^t_{ij}  \\
    \partial_{x_{i,p}} F =& \frac{1}{n}\partial_{x_{i,p}} f_i(x^t_{i,s},x^t_{j,p})
  \end{align}
  Update this with gradient
  \begin{equation}
    x_{i}^{t+1} = x_i^t - \eta \partial_{x_{i}} F(W^t;\lambda_{ij}^t, X^t).
  \end{equation}
  \item Compute gradient with respect to $\lambda_{ij}$
  \begin{align}
    \partial_{\lambda_{ij}} F = w_{ij} (x_{i,s} - x_{j,s}).
  \end{align}
  Update $\lambda_{ij}$ by gradient ascent
  \begin{equation}
    \lambda_{ij}^{t+1} = \lambda_{ij}^t + \gamma \partial_{\lambda_{ij}} F(W^t;\lambda_{ij}^t, X^{t+1})
  \end{equation}
  \item Update $W$ by Frank-Wolfe.
  \begin{itemize}
    \item Compute $\partial_{w_{ij}} F = \left<\lambda_{ij}, x_{i,s} - x_{j,s} \right> - \alpha$
    \item Minimize $\mathbf{s}^\top \nabla F(W)$ subject to $s_{ij} \in [0,1]$, $s_{ij}=s_{ji}$, $s_{ii}=1$.
    \item Set step size $\beta=2/(t+2)$ or alternative strategies.
    \item Update $W\leftarrow W + \beta(\mathbf{s} - W)$
  \end{itemize}
  Or in other words
  \begin{align}
    s_{ij}^{t+1} =& \tfrac{1}{2} \left(1+\text{sign}\left( - \partial_{w_{ij}} F(W^t; \lambda_{ij}^{t+1}, X^{t+1})\right) \right) \\
    w_{ij}^{t+1} =& w_{ij}^t + \beta(s_{ij}^{t+1} - w_{ij}^t)
  \end{align}
\end{enumerate}



\end{document}