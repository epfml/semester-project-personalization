\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amssymb,amsmath,amsthm}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{algorithm}
\usepackage{algpseudocode}  % http://mirror.kumi.systems/ctan/macros/latex/contrib/algorithmicx/algorithmicx.pdf
\usepackage{graphicx}

\usepackage{lipsum}         % for dummy text
\usepackage{xspace}         % for at the end of macros
\usepackage{xargs}          % defines \newcommandx
\usepackage{mathtools}
\usepackage{bm}
\usepackage[dvipsnames]{xcolor} % defines \textcolor
\usepackage{tabularx}       % like this for tables
\usepackage{wrapfig}        % left- and right-floating figures+tables
\usepackage[export]{adjustbox}


\usepackage{tikz}           % for drawing
\usepackage{ifthen}
\usepackage{enumitem}
\usepackage{cleveref}
\usetikzlibrary{patterns}
\usetikzlibrary{positioning}

\newcommand\includegraphicscrop[1]{%
\immediate\write18{pdfcrop -hires #1.pdf #1-crop.pdf}%
\includegraphics{#1-crop.pdf}%
}
     
\input{macros}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
% \newcommand{\gset}{\ensuremath{\cV_{\mathpzc{R}}}}
% \newcommand{\bset}{\ensuremath{\cV_{\mathpzc{B}}}}
\newcommand{\gset}{\ensuremath{{\mathpzc{G}}}}
\newcommand{\bset}{\ensuremath{{\mathpzc{B}}}}
\newcommand{\gcset}{\ensuremath{{\mathpzc{G_c}}}}
\newcommand{\bcset}{\ensuremath{{\mathpzc{B_c}}}}
\newcommand{\gfset}{\ensuremath{{\mathpzc{G_f}}}}
\newcommand{\bfset}{\ensuremath{{\mathpzc{B_f}}}}
\newcommand{\cpG}{{{\mathsf{G}}}}
\newcommand{\cpH}{{{\mathsf{H}}}}
\newcommand{\cpC}{{{\mathsf{C}}}}


\title{}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

%\begin{abstract}
%    
%\end{abstract}
\section{Introduction}

In the federated learning and decentralized learning, $n$ participants collaborate to train a global model $\xx$ over their joint objectives $\min_{\xx} \frac{1}{n}\sum_{i=1}^n f_i(\xx)$.
Compared to models trained on individual data silos, this model achieves overall better performance on dataset. 
There is no guarantee that it performs better than standalone training on some workers. 
Personalized federated learning is one way to address this problem. 
% 

\section{Shared setting}
\begin{assumption}[$L$-smoothness]\label{a:smooth}
  For $i\in[n]$, $f_i$ is $L$-smooth.
\end{assumption}
\begin{assumption}[Lower bound]\label{a:lower-bound}
  For $i\in[n]$, $f_i$ is lower bounded by $f_i^\star$.
\end{assumption}

\section{Dynamic graph and full gradient}

\subsection{Problem formulation}
In this section, we assume not all workers share same stationary points or minimizers. 
\begin{assumption}[Strong growth condition]\label{a:strong-growth}
  Let $c\subset[n]$ be the a subset of workers that share same stationary point. Then for $\xx\in\R^d$ and $i\in c$, we have
  \begin{align*}
    \norm*{\nabla f_i(\xx) - \nabla \bar{f}_c(\xx)}_2 \le M \norm*{\nabla \bar{f}_c(\xx)}_2.
  \end{align*}
\end{assumption}
The \Cref{a:strong-growth} indicates that when an iterate $\xx$ reaches the stationary point of $\bar{f}_c$, then it also reaches the stationary point of $f$ for all $i\in c$. The optimization objective is that
\begin{align*}
  \min_{X\in\mathbb{R}^{d\times n}}&  \frac{1}{n} \sum_{i=1}^n f_i(x_i) + \frac{\rho}{2} \sum_{i<j} w_{ij}\norm*{x_{i} - x_{j}}_2^2.
\end{align*}
We optimize the objective with gradient descent with initialization $\xx_i^0=\bar{\xx}^0$
\begin{equation}\label{eq:x:gd}
    \xx_i^{t+1}=\xx_i^t - \eta \left(
      \nabla f_i(\xx^t_i) + \rho \sum_{k=1}^n w^t_{ik}(\xx^t_i - \xx^t_k)
    \right).
\end{equation}
We update $w_{ij}^t$ with the following term
\begin{align*}
  w_{ij}^{t+1} = \text{sign}\left(\alpha - \norm{\xx^t_i - \xx^t_j}_2^2 \right)
\end{align*}

\subsection{Proof Sketch}
\textbf{Notations.} We define the following notations
\begin{itemize}
  \item Let $\mX=[\xx_1, \xx_2, \ldots, \xx_n]^\top \in\R^{n\times d}$ be the compact form of iterates.
  \item Let $\nabla F(\mX):= [\nabla f_1(\xx_1), \nabla f_2(\xx_2), \ldots, \nabla f_n(\xx_n)]^\top \in\R^{n\times d}$.
  \item Let $\mW^t$ be the mixing matrix at time $t$ and $\mD^t:=\text{Diag}(\mW^t\1)$. 
  \item Let $\mW^\star$ be the groundtruth mixing matrix.
  \item Let $d_\text{max}=\max\{D^\star\}$ be the size of largest cluster.
\end{itemize}
Then we update the iterates as follows
\begin{equation}
  \label{eq:X:iterate}
  \mX^{t+1} = (\mI - \eta\rho \left( \mD^t - \mW^t \right)) \mX^t - \eta \nabla F(\mX^t).
\end{equation}
\begin{lemma}[Basic properties]
  The following equality and inequalities hold true
  \begin{itemize}
    \item $(\mD^\star - \mW^\star)(\mD^\star - \mW^\star)=\mD^\star(\mD^\star - \mW^\star)=(\mD^\star - \mW^\star)\mD^\star$
    \item $\norm*{AB}_F \le \norm*{A}_2 \norm*{B}_F $
  \end{itemize}
\end{lemma}
Note that~\eqref{eq:x:gd} can be re-written as 
\begin{align*}
  \xx_i^{t+1}=\xx_i^t - \eta \left(
    \nabla f_i(\xx^t_i) + \rho \sum_{k=1}^n w^{\star}_{ik}(\xx^t_i - \xx^t_k)
    + \rho \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)
  \right).
\end{align*}
After averaging $i$ over the same cluster, we have
\begin{equation}\label{eq:xbar}
  \bar{\xx}_c^{t+1}=\bar{\xx}_c^t - \eta \left(
    \frac{1}{c}\sum_{i\in c}\nabla f_i(\xx^t_i)
    + \frac{\rho}{c}\sum_{i\in c} \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)
  \right).
\end{equation}
\begin{lemma}[Sufficient decrease]
  \label{lemma:sd}  
  Suppose $f_i$ are $L$-smooth, then by taking $\eta\le \frac{1}{L}$, we have 
  \begin{align*}
    \bar{f}_c\left(\bar{\xx}_c^{t+1}\right)
     \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
     - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2
     +  \frac{\eta L^2}{c}\sum_{i\in c} \norm*{ \xx^t_i - \bar{\xx}_c^{t} }_2^2
    % + \eta \norm*{\frac{\rho}{c}\sum_{i\in c} \sum_{k\notin c} w^t_{ik}(\xx^t_i - \xx^t_k)}_2^2.
    +\eta \norm*{\frac{\rho}{c}\sum_{i\in c} \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2.
  \end{align*}
\end{lemma}

Here are related equality
\begin{itemize}
  \item $\sum_c\sum_{i\in c} \norm*{ \xx^t_i - \bar{\xx}_c^{t} }_2^2=\norm*{(\mI - \mD^{-1}\mW^\star) \mX^{t}}_F^2$
  \item $\sum_c c\norm*{\frac{1}{c}\sum_{i\in c} \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2=\norm*{\mD^{-1}\mW^\star(\mD^\star - \mW^\star - \mD^t + \mW^t)\mX^t}_F^2$
  \item $\sum_c \sum_{i\in c}\norm*{ \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2=\norm*{(\mD^\star - \mW^\star - \mD^t + \mW^t)\mX^t}_F^2$
\end{itemize}
Let $\cE_i:= \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)$ be the Misclassification term of worker $i$.
\begin{lemma}[Misclassification error]
  Let $\cE:=\norm*{(\mD^\star - \mW^\star - \mD^t + \mW^t)\mX^t}_F^2$
  be the error incurred by misclassification
  \begin{align*}
    \cE
    \le 2 \cE_\text{ex} + 2  \cE_\text{in}.
  \end{align*}
  where $\cE_\text{ex}=\sum_c \sum_{i\in c} c \sum_{k\in c \& w_{ik}^t = 0}  \norm*{ \xx^t_i - \xx^t_k}_2^2$ and $\cE_\text{in} = \alpha^t n (n-c)^2 $.
\end{lemma}
Note that as we initialize all models to be the same, the $\cE_\text{ex}$ is small in the beginning; as we choose $\alpha^t=\cO(\frac{1}{t})$, the $\cE_\text{in}$ is gradually decreasing. The inclusion error eventually vanishes due to heterogeneity across clusters, in which case we can stop decreasing $\alpha^t$. The exclusion error is bounded by generalized strong-growth condition.
\begin{lemma}
  
\end{lemma}
\begin{proof}
  \begin{align*}
    \norm*{\xx^{t+1}_i - \xx^{t+1}_k}_2^2
    =& \norm*{\xx^{t}_i - \xx^{t}_k - \eta(\nabla f_i(\xx_i^t) - \nabla f_k(\xx_k^t))
    - \eta \rho c (\xx_i^t - \xx_k^t)
    - \eta \rho (\cE_i - \cE_k)
    }_2^2 \\
    \le& (1-\eta\rho c) \norm*{\xx^{t}_i - \xx^{t}_k}_2^2
    + \eta\rho c \norm*{\frac{1}{\rho c}\left(\nabla f_i(\xx_i^t) - \nabla f_k(\xx_k^t)\right) + \frac{1}{c}(\cE_i - \cE_k)}_2^2.
  \end{align*}
  Note that
  \begin{align*}
    \norm*{\nabla f_i(\xx_i^t) - \nabla f_k(\xx_k^t)}_2^2
    \le& \norm*{\nabla f_i(\xx_i^t) \pm \nabla f_i(\bar{\xx}_c^t) \pm \nabla \bar{f}_c(\bar{\xx}_c^t)  \pm \nabla f_k(\bar{\xx}_c^t) - \nabla f_k(\xx_k^t)}_2^2 \\
    \le& 4 L^2 \norm*{\xx_i^t - \bar{\xx}_c^t}_2^2 + 4 L^2 \norm*{\xx_k^t - \bar{\xx}_c^t}_2^2 + 8 M^2 \norm{\nabla \bar{f}_c(\bar{\xx}_c^t)}_2^2.
  \end{align*}
  Then
  \begin{align*}
    \frac{1}{c^2}\sum_{i\in c} \sum_{k\in c}\norm*{\nabla f_i(\xx_i^t) - \nabla f_k(\xx_k^t)}_2^2 \le 8L^2 \frac{1}{c}\sum_{k\in c} \norm*{\xx_k^t - \bar{\xx}_c^t}_2^2 + 8 M^2 \norm{\nabla \bar{f}_c(\bar{\xx}_c^t)}_2^2.
  \end{align*}
  Then
  \begin{align*}
    \frac{1}{c^2}\sum_{i\in c} \sum_{k\in c} \norm*{\xx^{t+1}_i - \xx^{t+1}_k}_2^2
    \le& (1-\eta\rho c) \frac{1}{c^2}\sum_{i\in c} \sum_{k\in c} \norm*{\xx^{t}_i - \xx^{t}_k}_2^2 \\
    &+ 2 \frac{\eta}{\rho c} \left(8L^2 \frac{1}{c}\sum_{k\in c} \norm*{\xx_k^t - \bar{\xx}_c^t}_2^2 + 8 M^2 \norm{\nabla \bar{f}_c(\bar{\xx}_c^t)}_2^2 \right) \\
    &+ 2 \frac{\eta\rho}{c} \frac{1}{c^2}\sum_{i\in c} \sum_{k\in c}\norm*{\cE_i - \cE_k}_2^2.
  \end{align*}
\end{proof}

\begin{lemma}[Consensus distance]
  The consensus distance measures the worker model's distance to the center of its own cluster, i.e. $\norm*{(\mI - \mD^{-1}\mW^\star) \mX}_F^2$ in the compact form.
\end{lemma}

\begin{proof}
The distance to their own center is
\begin{align*}
  % \label{eq:X:cd}
  (\mD^{\star} - \mW^\star) \mX^{t+1}
  = & (\mD^\star - \mW^\star)(\mI - \eta\rho \left( \mD^t - \mW^t \right)) \mX^t
  -\eta(\mD^\star - \mW^\star) \nabla F(\mX^t) \\
  = & (\mI - \eta\rho \left( \mD^\star - \mW^\star \right)) (\mD^\star - \mW^\star) \mX^t \\
  & + \eta\rho(\mD^\star - \mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t \\
  & -\eta(\mD^\star - \mW^\star) \nabla F(\mX^t) \\
  = & (\mI - \eta\rho \mD^\star ) (\mD^\star - \mW^\star) \mX^t \\
  & + \eta\rho(\mD^\star - \mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t \\
  & -\eta(\mD^\star - \mW^\star) \nabla F(\mX^t).
  \end{align*}
Multiply with $\mD^{-1}$ to both sides yield
\begin{align*}
  (\mI - \mD^{-1}\mW^\star) \mX^{t+1}
  = & (\mI - \eta\rho \mD^\star ) (\mI - \mD^{-1}\mW^\star) \mX^t \\
  & + \eta\rho(\mI - \mD^{-1}\mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t \\
  & -\eta(\mI - \mD^{-1}\mW^\star) \nabla F(\mX^t).
\end{align*}


Applying Frobenius norm to the above equation, we have 
\begin{align*}
  &\norm*{(\mI - \mD^{-1}\mW^\star) \mX^{t+1}}_F^2 \\
  =& 3\norm*{(\mI - \eta\rho \mD^\star ) (\mI - \mD^{-1}\mW^\star) \mX^t}_F^2
   + 3\norm*{\eta\rho(\mI - \mD^{-1}\mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t}_F^2 \\
    & + 3\norm*{\eta(\mI - \mD^{-1}\mW^\star) \nabla F(\mX^t)}_F^2\\
  =& 3\norm*{I - \eta\rho \mD^\star }_2^2  \norm*{(\mI - \mD^{-1}\mW^\star) \mX^{t}}_F^2
  + 3\norm*{\eta\rho(\mI - \mD^{-1}\mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t}_F^2 \\
  & + 3\norm*{\eta(\mI - \mD^{-1}\mW^\star) \nabla F(\mX^t)}_F^2\\ 
  \le& (1-\eta\rho d_\text{max})  \norm*{(\mI - \mD^{-1}\mW^\star) \mX^{t}}_F^2
  + 3\norm*{\eta\rho(\mI - \mD^{-1}\mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t}_F^2 \\
  & + 3\norm*{\eta(\mI - \mD^{-1}\mW^\star) \nabla F(\mX^t)}_F^2
\end{align*}
where we use $\rho\ge \frac{2}{3\eta d_\text{max}}$. The second term can be bounded as follows
\begin{align*}
  \norm*{(\mI - \mD^{-1}\mW^\star)(\mD^\star - \mW^\star - \mD^t + \mW^t) \mX^t}_F^2 \le \norm{\mI - \mD^{-1}\mW^\star}_2^2 \cE.
\end{align*}
The last term can be bounded as follows
\begin{align*}
  \norm*{(\mI - \mD^{-1}\mW^\star) \nabla F(\mX^t)}_F^2
\end{align*}
\end{proof}


\newpage
\appendix
\section{Proofs}
\begin{proof}
  Apply L-smoothness to each function $f_i$, we have
  \begin{align*}
    f_i\left(\bar{\xx}_c^{t+1}\right) \le f_i\left(\bar{\xx}_c^{t}\right) 
    + \left\langle \nabla f_i\left(\bar{\xx}_c^{t}\right), \bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}\right\rangle
    + \frac{L}{2} \norm*{\bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}}_2^2.
  \end{align*}
  Average the above inequality over $i\in c$, we have
  \begin{align*}
    \bar{f}_c\left(\bar{\xx}_c^{t+1}\right) \le \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
    + \left\langle \nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right), \bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}\right\rangle
    + \frac{L}{2} \norm*{\bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}}_2^2.
  \end{align*}
  Expand the intermediate term, we have 
  \begin{align*}
    \left\langle \nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right), \bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}\right\rangle=& - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2 - \frac{1}{2\eta} \norm*{ \bar{\xx}_c^{t+1} - \bar{\xx}_c^{t} }_2^2 \\
    &+ \frac{\eta}{2} \norm*{\frac{\bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}}{\eta} - \nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right)}_2^2.
  \end{align*}
  Then by having $\eta\le \frac{1}{L}$
  \begin{align*}
    \bar{f}_c\left(\bar{\xx}_c^{t+1}\right) \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
    - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2
    + \frac{\eta}{2} \norm*{\frac{\bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}}{\eta} - \nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right)}_2^2 \\
    &- \frac{1-L\eta}{2\eta} \norm*{ \bar{\xx}_c^{t+1} - \bar{\xx}_c^{t} }_2^2 \\
    \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
    - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2
    + \frac{\eta}{2} \norm*{\frac{\bar{\xx}_c^{t+1} - \bar{\xx}_c^{t}}{\eta} - \nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right)}_2^2
  \end{align*}
  Using \eqref{eq:xbar} we have that
  \begin{align*}
    \bar{f}_c\left(\bar{\xx}_c^{t+1}\right)
    \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
    - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2 \\
    &+ \frac{\eta}{2} \norm*{\frac{1}{c}\sum_{i\in c}\nabla f_i(\xx^t_i)
     - \nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) + \frac{\rho}{c}\sum_{i\in c} \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2 \\
     \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
     - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2
     + \eta \norm*{\frac{1}{c}\sum_{i\in c} \left(\nabla f_i(\xx^t_i) - \nabla f_i\left(\bar{\xx}_c^{t}\right)\right) }_2^2 \\
      & + \eta \norm*{\frac{\rho}{c}\sum_{i\in c} \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2.
  \end{align*}
  Using the $L$-smoothness of $f_i$
  \begin{align*}
    \bar{f}_c\left(\bar{\xx}_c^{t+1}\right)
     \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
     - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2
     +  \frac{\eta L^2}{c}\sum_{i\in c} \norm*{ \xx^t_i - \bar{\xx}_c^{t} }_2^2 \\
      & + \eta \norm*{\frac{\rho}{c}\sum_{i\in c} \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2.
  \end{align*}
  % By the symmetry of $w^t_{ik}$ and $w^\star_{ik}$, and $w^\star_{ik}=0$ for $i\in c$, $j\notin c$
  % \begin{align*}
  %   \bar{f}_c\left(\bar{\xx}_c^{t+1}\right)
  %    \le& \bar{f}_c\left(\bar{\xx}_c^{t}\right) 
  %    - \frac{\eta}{2} \norm*{\nabla \bar{f}_c\left(\bar{\xx}_c^{t}\right) }_2^2
  %    +  \frac{\eta L^2}{c}\sum_{i\in c} \norm*{ \xx^t_i - \bar{\xx}_c^{t} }_2^2
  %   + \eta \norm*{\frac{\rho}{c}\sum_{i\in c} \sum_{k\notin c} w^t_{ik}(\xx^t_i - \xx^t_k)}_2^2.
  % \end{align*}
\end{proof}
\subsection{Misclassification Error}
\begin{proof}
  \begin{align*}
    \cE=&\sum_c \sum_{i\in c}\norm*{ \sum_{k=1}^n (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2 \\
    =&\sum_c \sum_{i\in c}\norm*{ \sum_{k\in c} (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k) + \sum_{k\notin c} (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2 \\
    \le& 2\underbrace{\sum_c \sum_{i\in c}\norm*{ \sum_{k\in c} (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2}_\text{Exclusion Error $\cE_\text{ex}$} +2\underbrace{\sum_c \sum_{i\in c}\norm*{\sum_{k\notin c} (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2}_\text{Inclusion Error $\cE_\text{in}$}
  \end{align*}
  The exclusion error
  \begin{align*}
    \cE_\text{ex}=&\sum_c \sum_{i\in c}\norm*{ \sum_{k\in c} (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2 \\
    \le& \sum_c \sum_{i\in c} \left(\sum_{k\in c} (w^{\star}_{ik}-w^t_{ik})\right) \left(\sum_{k\in c} (w^{\star}_{ik}-w^t_{ik}) \norm*{ \xx^t_i - \xx^t_k}_2^2\right) \\
    \le& \sum_c \sum_{i\in c} c \sum_{k\in c \& w_{ik}^t = 0}  \norm*{ \xx^t_i - \xx^t_k}_2^2.
  \end{align*}
  This may be further bounded as a function of $\alpha_t$ (but we may not use it)
  \begin{align*}
    \cE_\text{ex}=& \sum_c \sum_{i\in c} c \frac{\left(\sum_{k\in c \& w_{ik}^t = 0}  \norm*{ \xx^t_i - \xx^t_k}_2^2\right)^2}{\alpha^t}
    \text{ Or }  \frac{\left(\sum_c \sum_{i\in c} c \sum_{k\in c \& w_{ik}^t = 0}  \norm*{ \xx^t_i - \xx^t_k}_2^2\right)^2}{\alpha^t}.
  \end{align*}

  On the other hand, the inclusion error can be bounded as follows
  \begin{align*}
    \cE_\text{in}=&\sum_c \sum_{i\in c}\norm*{\sum_{k\notin c} (w^t_{ik}-w^{\star}_{ik})(\xx^t_i - \xx^t_k)}_2^2 \\
    \le&  \sum_c \sum_{i\in c} (n-c)\sum_{k\notin c\& w_{ik}^t=1} \norm*{\xx^t_i - \xx^t_k}_2^2 \\
    \le& \alpha^t n (n-c)^2.
  \end{align*}
\end{proof}


\end{document}